<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: 'Lato', sans-serif;">Not all images are worth a thousand words!</h1>
      <h4 style="font-family: 'Lato', sans-serif;">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: 'Lato', sans-serif;">Visual Linguists</h4>


      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
            <img src="./files/image.png" alt="Profile Image">
            
            
          </div>
          <p>
                        
            Koustav Banerjee
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="./files/profile.png">
            
          </div>
          <p>
            
            Hardik Gupta
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="./files/Lin.png">            
            
          </div>
          <p>
            Lin Xie
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href="./files/NLP_Final_Report.pdf"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

    <!-- ## One or two sentences on the motivation behind the problem you are solving. One or two sentences describing the approach you took. One or two sentences on the main result you obtained. -->
<p>
  Human memory for abstract visuals is notably poorer than for real-world scenes. We investigated whether this discrepancy stems from the difficulty in verbally labeling abstract content. We utilize a set of abstract images that vary in their complexities and human memory performance on those images to test our hypothesis. Using LLMs to generate captions for these abstract images of varying complexity, we found that caption complexity mirrored visual complexity. We built several simple models to predict human memory performance on abstract images and found that a predictive model based on the generated captions' semantic content performed competitively to a model relying solely on visual features. This study suggests that our ability to verbalize and name what we are seeing might indeed underpin our ability to remember a visual scene.
</p>

<hr>

<h2 id="teaser">Schematic Figure</h2>

<p>A figure that conveys the schematic diagram behind the project or the main application being addressed. This figure is from <a href="https://arxiv.org/pdf/2210.07469">StyLEx</a>.</p>

<p class="sys-img"><img src="./files/Schematic.png" alt="imgname"></p>


<!-- <h3 id="the-timeline-and-the-highlights">Any subsection</h3> -->

<!-- <p>If you need to explain more about your figure</p> -->

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>

<p>
<b>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</b>
</p>
<p>
Basically trying to see if visual scenes may be represented linguistically when stored in human memory or are imagined back. We wanted to find out why some pictures are harder to remember than others. Specifically, we asked if pictures that are more abstract and difficult to describe in words are also harder to memorize. Our goal was to see if there's a link between how complex an image is and how challenging it is to remember it.
</p>

<p>
<b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
It involves generating captions for abstract images and no LLM model to our knowledge is tailored for it. Most psychological and neuroscience experiments use synthetic, non-natural images so obtaining any semantic information on them is impossible as the computational models don't focus on them. As such this involves careful prompt engineering to use existing LLMs to generate captions for non-natural images.<p>

<p>
<b>Who cares? If you are successful, what difference will it make?</b>
</p>
<p>
Psychologists and neuroscientists, specifically computational neuroscientists and perception researchers as this can provide an novel way to understand the interplay between visual memory and our thoughts which are linguistic. It can also benefit VLM research in the computer science community,.</p>

<hr>

<h2 id="approach">Approach</h2>

<p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
  We input the abstract images into GPT with various prompts (e.g., "Describe the image in detail"). For each image, we obtained captions like "A complex interplay of swirling colors and shapes forming an abstract pattern." We evaluated these captions using Likert scales and ROUGE-1 scores, selecting prompts that produced captions closest to human descriptions.
</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
  No modern LLM handles non-natural and synthetic data well for image captioning. As a result even careful prompt enginnering cannot generate very well defined captions, that captures what a human would look for. As a result we had to use human evaluations with reference annotations for it. Automatic evaluations like ROGUE scores compare similarities between a reference text and a generated text. However, since this is a visual task, a human generated Likert score would be able to understand how well the captions describe the visual image. As a result human intervation was necessary. The prompt enginenring took multiple tries.
<hr>
    
<h2 id="results">Results</h2>
<p>
<b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b>
</p>
<p>Recent work has shown that as image complexity
  increases, human memory performance declines
  for abstract images</p>
<br>
<div style="text-align: center;">
<img style="height: 400px;" alt="" src="./files/Model Comparison.png">
</div>

<p>A UMAP (Uniform Manifold Approximation
  and Projection) of the captions of select group of images was used to cluster
  captions generated for abstract images by their caption complexity. The UMAP demonstrates clear
  separation among captions based on their associated image complexities. This separation indicates that the captions, reflect underlying patterns of image complexity that humans
  intuitively perceive</p>
<br>
<div style="text-align: center;">
<img style="height: 400px;" alt="" src="./files/UMAP_Final.png">
</div>

<p>We also compared the performance of models
  predicting memory performance using different
  semantic complexity metrics. A
  K-Fold cross-validation approach was employed
  to rigorously compare a semantic memorability
  model (based on a linear combination of perplexity
  and concreteness) with a purely visual complexity
  model. Remarkably, the semantic model proved
  competitive with the visual complexity model, underscoring the importance of semantic factors in
  determining memorability.</p>

<br><br>

<hr>



<h2 id="conclusion">Conclustion and Future Work</h2>
<p>
  Overall, the results strongly support the idea that se-
mantic descriptions aid visual memory. They reveal
a robust connection between image complexity, se-
mantic representation, and memory performance,
shedding light on why natural scenes are easier to
remember than abstract ones. The implications of
this study extend to areas such as AI-driven image
captioning, memory training, and understanding
the cognitive basis of visual and semantic integra-
tion.
In this study, we quantitatively analyzed hu-
man visual memory performance through semantic
means by leveraging metrics such as perplexity and
concreteness. These metrics serve as proxies for
the difficulty of generating semantic descriptions,
revealing that abstract images with higher visual
complexity lack clear semantic cues, leading to in-
creased memory recall errors. This supports the
hypothesis that humans rely on both visual and
semantic representations to encode and retrieve
memories, with the absence of semantic anchors in
abstract images impairing memory performance.
Our UMAP analysis demonstrated that captions
for less complex images cluster tightly, reflecting straightforward semantic structures, while captions
for more complex images are dispersed, indicating
greater variability and difficulty in interpretation.
This suggests that semantic properties are closely
tied to visual complexity, offering new insights
into how image semantics influence cognitive pro-
cesses.</p>
  

<hr>


  </div>
  


</body></html>
