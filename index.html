<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html lang=" en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>

  <link rel="stylesheet" href="./files/bulma.min.css" />

  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">


  <base href="." target="_blank"></head>


<body>
  <div>
    <div class="wrapper">
      <h1 style="font-family: 'Lato', sans-serif;">Not all images are worth a thousand words!</h1>
      <h4 style="font-family: 'Lato', sans-serif;">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
      <h4 style="font-family: 'Lato', sans-serif;">Visual Linguists</h4>


      <div class="authors-wrapper">
        
        <div class="author-container">
          <div class="author-image">
                        
            <img src="./files/image.png" alt="Profile Image">
            
            
          </div>
          <p>
                        
            Koustav Banerjee
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
            <img src="./files/profile.png">
            
          </div>
          <p>
            
            Hardik Gupta
            
          </p>
        </div>
        
        <div class="author-container">
          <div class="author-image">
            
              <img src="./files/Lin.png">            
            
          </div>
          <p>
            Lin Xie
          </p>
        </div>
        
      </div>

      <br/>

      <div class="authors-wrapper">
        <div class="publication-links">
          <!-- Github link -->
          <span class="link-block">
            <a
              href="./files/NLP_Final_Report.pdf"
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Final Report</span>
            </a>
          </span>
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Code</span>
            </a>
          </span>      
          <span class="link-block">
            <a
              href=""
              target="_blank"
              class="external-link button is-normal is-rounded is-dark is-outlined"
            >
            <span>Model Weights</span>
            </a>
          </span>              
        </div>
      </div>


    </div>
  </div>





  
  


  <div class="wrapper">
    <hr>
    
    <h2 id="abstract">Abstract</h2>

    <!-- ## One or two sentences on the motivation behind the problem you are solving. One or two sentences describing the approach you took. One or two sentences on the main result you obtained. -->
<p>
  Human memory for abstract visuals is notably poorer than for real-world scenes. We investigated whether this discrepancy stems from the difficulty in verbally labeling abstract content. We utilize a set of abstract images that vary in their complexities and human memory performance on those images to test our hypothesis. Using LLMs to generate captions for these abstract images of varying complexity, we found that caption complexity mirrored visual complexity. We built several simple models to predict human memory performance on abstract images and found that a predictive model based on the generated captions' semantic content performed competitively to a model relying solely on visual features. This study suggests that our ability to verbalize and name what we are seeing might indeed underpin our ability to remember a visual scene.
</p>

<hr>

<h2 id="teaser">Schematic Figure</h2>

<p>A figure that conveys the schematic diagram behind the project or the main application being addressed. This figure is from <a href="https://arxiv.org/pdf/2210.07469">StyLEx</a>.</p>

<p class="sys-img"><img src="./files/Schematic.png" alt="imgname"></p>


<!-- <h3 id="the-timeline-and-the-highlights">Any subsection</h3> -->

<!-- <p>If you need to explain more about your figure</p> -->

<hr>

<h2 id="introduction">Introduction / Background / Motivation</h2>

<p>
<b>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</b>
</p>
<p>
Basically trying to see if visual scenes may be represented linguistically when stored in human memory or are imagined back. We wanted to find out why some pictures are harder to remember than others. Specifically, we asked if pictures that are more abstract and difficult to describe in words are also harder to memorize. Our goal was to see if there's a link between how complex an image is and how challenging it is to remember it.
</p>

<p>
<b>How is it done today, and what are the limits of current practice?</b>
</p>
<p>
It involves generating captions for abstract images and no LLM model to our knowledge is tailored for it. Most psychological and neuroscience experiments use synthetic, non-natural images so obtaining any semantic information on them is impossible as the computational models don't focus on them. As such this involves careful prompt engineering to use existing LLMs to generate captions for non-natural images.<p>

<p>
<b>Who cares? If you are successful, what difference will it make?</b>
</p>
<p>
Psychologists and neuroscientists, specifically computational neuroscientists and perception researchers as this can provide an novel way to understand the interplay between visual memory and our thoughts which are linguistic. It can also benefit VLM research in the computer science community,.</p>

<hr>

<h2 id="approach">Approach</h2>

<p>
<b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b>
</p>

<p>
  We input the abstract images into GPT with various prompts (e.g., "Describe the image in detail"). For each image, we obtained captions like "A complex interplay of swirling colors and shapes forming an abstract pattern." We evaluated these captions using Likert scales and ROUGE-1 scores, selecting prompts that produced captions closest to human descriptions.
</p>

<p>
<b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b>
</p>

<p>
  No modern LLM handles non-natural and synthetic data well for image captioning. As a result even careful prompt enginnering cannot generate very well defined captions, that captures what a human would look for. As a result we had to use human evaluations with reference annotations for it. Automatic evaluations like ROGUE scores compare similarities between a reference text and a generated text. However, since this is a visual task, a human generated Likert score would be able to understand how well the captions describe the visual image. As a result human intervation was necessary. The prompt enginenring took multiple tries.
<hr>
    
<h2 id="results">Results</h2>
<p>
<b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b>
</p>
<p>Recent work has shown that as image complexity
  increases, human memory performance declines
  for abstract images</p>
<br>
<div style="text-align: center;">
<img style="height: 400px;" alt="" src="./files/Model Comparison.png">
</div>

<p>A UMAP (Uniform Manifold Approximation
  and Projection) (McInnes et al., 2018) of the captions of select group of images was used to cluster
  captions generated for abstract images by their caption complexity. The UMAP demonstrates clear
  separation among captions based on their associated image complexities (See Figure 6). This separation indicates that the captions, reflect underlying patterns of image complexity that humans
  intuitively perceive</p>
<br>
<div style="text-align: center;">
<img style="height: 400px;" alt="" src="./files/UMAP_Final.png">
</div>

<p>We also compared the performance of models
  predicting memory performance using different
  semantic complexity metrics (See Figure 7). A
  K-Fold cross-validation approach was employed
  to rigorously compare a semantic memorability
  model (based on a linear combination of perplexity
  and concreteness) with a purely visual complexity
  model. Remarkably, the semantic model proved
  competitive with the visual complexity model, underscoring the importance of semantic factors in
  determining memorability.</p>

<br><br>

<hr>



<h2 id="conclusion">Conclustion and Future Work</h2>
<p>

  So far, we have succefully able to prompt enginner for captioning 
  synthetic images which means that we can use LLMs to get captions 
  for larger datasets and use it as a human proxy, saving a lot of time required for manual annotation. 

</p> <p>
  Once we identified the best-performing prompts, we used GPT to generate captions for all the images in our dataset.We proceeded to analyze the complexity of these captions using readability scores likeFleschâ€™s Reading Ease and measures of syntactic complexity. Our aim was to determineif there's a positive correlation between the linguistic complexity of the captions and theestablished image complexity scores. We believed this approach would be successfulbecause it integrates human insights with advanced NLP techniques to produce andevaluate descriptions of abstract images. By quantifying caption complexity andcomparing it to memory performance, we hoped to uncover patterns that explain whycertain images are harder to remember. What's novel in our approach is the applicationof GPT and prompt engineering specifically tailored to artificially generated abstractimages, as well as the combination of human and machine-generated captions to refineour methods. This interdisciplinary strategy bridges gaps between visual perception,linguistic representation, and memory research.</p>
</p>
  

<hr>


  </div>
  


</body></html>
